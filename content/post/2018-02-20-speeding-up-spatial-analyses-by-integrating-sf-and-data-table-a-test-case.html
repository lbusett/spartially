---
title: 'Speeding up spatial analyses by integrating `sf` and `data.table`: a test
  case'
author: Lorenzo Busetto
date: '2018-02-20'
slug: speeding-up-spatial-analyses-by-integrating-sf-and-data-table-a-test-case
categories:
  - data.table
  - R
  - sf
  - spatial processing
tags:
  - R
---



<div id="the-problem" class="section level2">
<h2>The problem</h2>
<p>Last week, I replied to <a href="https://stackoverflow.com/questions/48650274/spatial-efficient-way-of-finding-all-points-within-x-meters-of-a-point">this interesting question</a> posted by <span class="citation">@Tim_K</span> over stackoverflow. He was seeking
efficient solutions to identify all points falling within a maximum distance of
xx meters with respect to each single point in a spatial points dataset.</p>
<p>If you have a look at the thread, you will see that a simple solution based on
creating a “buffered” polygon dataset beforehand and then intersecting it with
the original points is quite fast for “reasonably sized” datasets, thanks to sf
spatial indexing capabilities which reduce the number of the required comparisons
to be done (See <a href="http://r-spatial.org/r/2017/06/22/spatial-index.html" class="uri">http://r-spatial.org/r/2017/06/22/spatial-index.html</a>). In practice,
something like this:</p>
<pre class="r"><code># create test data: 50000 uniformly distributed points on a &quot;square&quot; of 100000
# metres
maxdist &lt;- 500
pts     &lt;- data.frame(x = runif(50000, 0, 100000),
                      y = runif(50000, 0, 100000),
                      id = 1:50000) %&gt;%
  sf::st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;))
# create buffered polygons
pts_buf &lt;- sf::st_buffer(pts, maxdist)
# Find points within 500 meters wrt each point
int &lt;- sf::st_intersects(pts_buf, pts)
int</code></pre>
<pre><code>## Sparse geometry binary predicate list of length 50000, where the predicate was `intersects&#39;
## first 10 elements:
##  1: 1, 39364, 43452
##  2: 2, 24400, 26773, 27460
##  3: 3, 26700, 32063, 38651, 40326
##  4: 4, 5351, 6136, 12632, 25758, 29705
##  5: 5, 6423, 7148, 40104
##  6: 6, 677, 1603, 10881, 14026, 16526, 25497, 29151
##  7: 7, 3291, 6757, 23374, 26785, 38543
##  8: 8, 1473, 28511, 31698
##  9: 9, 11200, 18048, 20814, 32992
##  10: 10, 1763, 15291, 31088, 37014</code></pre>
<p>However, this starts to have problems over really large datasets, because the total
number of comparisons to be done still rapidly increase besides the use of spatial
indexes. A test done by changing the number of points in the above example in the
range 25000 - 475000 shows for example this kind of behavior, for two different
values of maxdist (500 and 2000 m):</p>
<p><img src="/img/speeding_up/Rplot01.png" /></p>
<p>On the test dataset, the relationships are almost perfectly quadratic (due to the
uniform distribution of points). Extrapolating them to the 12 Million points dataset
of the OP, we would get an execution time of about 14 hours for maxdist = 500, and
a staggering 3.5 days formaxdist = 2000. Still doable, but not ideal…</p>
<p>My suggestion to the OP was therefore to “split” the points in chunks based on the
x-coordinate and then work on a per-split basis, eventually assigning each chunk
to a different core within a parallellized cycle.</p>
<p>In the end, I got curious and decided to give it a go to see what kind of performance
improvement it was possible to obtain with that kind of approach. You can find results
of some tests below.</p>
</div>
<div id="a-possible-solution-speeding-up-computation-by-combining-data.table-and-sf_intersect" class="section level1">
<h1>A (possible) solution: Speeding up computation by combining <code>data.table</code> and <code>sf_intersect</code></h1>
<p><br>
The idea here is to use a simple divide-and-conquer approach.</p>
<p>We first split the total spatial extent of the dataset in a certain number of regular quadrants. We then iterate over the quadrants and for each one we:</p>
<ul>
<li>Extract the points contained into the quadrant and apply a buffer to them;</li>
<li>Extract the points contained in a slightly larger area, computed by expanding
the quadrant by an amount equal to the maximum distance for which we want to
identify the “neighbors”;</li>
<li>Compute and save the intersection between the buffered points and the points
contained in the “expanded” quadrant.</li>
</ul>
<p>“Graphically”, this translates to exploring the dataset like this:</p>
<p><img src="/img/speeding_up/animation2.gif" /></p>
<p>, where the points included in the current “quadrant” are shown in green and the additional points needed to perform the analysis for that quadrant are shown in red.</p>
<p>Provided that the subsetting operations do not introduce an excessive overhead (i.e., they are fast enough…) this should provide a performance boost, because it should consistently reduce the total number of comparisons to be done.</p>
<p>Now, every “R” expert will tell you that if you need to perform fast subsetting over large datasets the way to go is to use properly indexeddata.tables, which provide lightning-speed subsetting capabilities.</p>
<p>So, let’s see how we could code this in a functions:</p>
<pre class="r"><code>points_in_distance &lt;- function(in_pts,
                               maxdist,
                               ncuts = 10) {

  require(data.table)
  require(sf)
  # convert points to data.table and create a unique identifier
  pts &lt;-  data.table(in_pts)
  pts &lt;- pts[, or_id := 1:dim(in_pts)[1]]

  # divide the extent in quadrants in ncuts*ncuts quadrants and assign each
  # point to a quadrant, then create the index over &quot;x&quot; to speed-up
  # the subsetting
  range_x  &lt;- range(pts$x)
  limits_x &lt;-(range_x[1] + (0:ncuts)*(range_x[2] - range_x[1])/ncuts)
  range_y  &lt;- range(pts$y)
  limits_y &lt;- range_y[1] + (0:ncuts)*(range_y[2] - range_y[1])/ncuts
  pts[, `:=`(xcut =  as.integer(cut(x, ncuts, labels = 1:ncuts)),
             ycut = as.integer(cut(y, ncuts, labels = 1:ncuts)))]  %&gt;%
    setkey(x)

  results &lt;- list()
  count &lt;- 0
  # start cycling over quadrants
  for (cutx in seq_len(ncuts)) {

    # get the points included in a x-slice extended by `maxdist`, and build
    # an index over y to speed-up subsetting in the inner cycle
    min_x_comp    &lt;- ifelse(cutx == 1,
                            limits_x[cutx],
                            (limits_x[cutx] - maxdist))
    max_x_comp    &lt;- ifelse(cutx == ncuts,
                            limits_x[cutx + 1],
                            (limits_x[cutx + 1] + maxdist))
    subpts_x &lt;- pts[x &gt;= min_x_comp &amp; x &lt; max_x_comp] %&gt;%
      setkey(y)

    for (cuty in seq_len(ncuts)) {
      count &lt;- count + 1

      # subset over subpts_x to find the final set of points needed for the
      # comparisons
      min_y_comp  &lt;- ifelse(cuty == 1,
                            limits_y[cuty],
                            (limits_y[cuty] - maxdist))
      max_y_comp  &lt;- ifelse(cuty == ncuts,
                            limits_y[cuty + 1],
                            (limits_y[cuty + 1] + maxdist))
      subpts_comp &lt;- subpts_x[y &gt;= min_y_comp &amp; y &lt; max_y_comp]

      # subset over subpts_comp to get the points included in a x/y chunk,
      # which &quot;neighbours&quot; we want to find. Then buffer them by maxdist.
      subpts_buf &lt;- subpts_comp[ycut == cuty &amp; xcut == cutx] %&gt;%
        sf::st_as_sf() %&gt;% 
        sf::st_buffer(maxdist)

      # retransform to sf since data.tables lost the geometric attrributes
      subpts_comp &lt;- sf::st_as_sf(subpts_comp)

      # compute the intersection and save results in a element of &quot;results&quot;.
      # For each point, save its &quot;or_id&quot; and the &quot;or_ids&quot; of the points within &quot;dist&quot;
      inters &lt;- sf::st_intersects(subpts_buf, subpts_comp)

      # save results
      results[[count]] &lt;- data.table(
        id = subpts_buf$or_id,
        int_ids = lapply(inters, FUN = function(x) subpts_comp$or_id[x]))
    }
  }
  data.table::rbindlist(results)
}</code></pre>
<p>The function takes as input a points sf object, a target distance and a number of “cuts” to use to divide the extent in quadrants, and provides in output a data frame in which, for each original point, the “ids” of the points within maxdist are reported in the int_ids list column.</p>
<p>Now, let’s see if this works:</p>
<pre class="r"><code>pts &lt;- data.frame(x = runif(20000, 0, 100000),
                  y = runif(20000, 0, 100000),
                  id = 1:20000) %&gt;%
  st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;), remove = FALSE)
maxdist &lt;- 2000
out &lt;- points_in_distance(pts, maxdist = maxdist, ncut = 10)
head(out)</code></pre>
<pre><code>##       id                                 int_ids
## 1:  5830  5830, 9068,10102, 5782,10062, 1188,...
## 2:  9068  5830, 9068,10102, 5782, 1188,15701,...
## 3: 10102  5830, 9068,10102, 5782,10062, 1188,...
## 4:  5989  5989, 7085,18143,  209, 5751, 5130,...
## 5:  5782  5830, 9068,10102, 5782,10062, 1188,...
## 6: 10062  5830,10102, 5782,10062,17566,15701,...</code></pre>
<pre class="r"><code># get a random point
sel_id  &lt;- sample(pts$id,1)
pt_sel  &lt;- pts[sel_id, ]
pt_buff &lt;- pt_sel %&gt;%  sf::st_buffer(maxdist)
# get ids of points within maxdist
id_inters &lt;- unlist(out[id == sel_id, ]$int_ids)
pt_inters &lt;- pts[id_inters,]

#plot results
plot &lt;- ggplot(pt_buff)  + theme_light() +
  geom_point(data = pts, aes(x = x, y = y), size = 1) +
  geom_sf(col = &quot;blue&quot;, size = 1.2, fill = &quot;transparent&quot;) +
  geom_sf(data = pt_inters, col = &quot;red&quot;, size = 1.5) +
  geom_point(data = pt_sel, aes(x = x, y = y), size = 2, col = &quot;green&quot;) +
  xlim(st_bbox(pt_buff)[1] - maxdist, st_bbox(pt_buff)[3] + maxdist) +
  ylim(st_bbox(pt_buff)[2] - maxdist, st_bbox(pt_buff)[4] + maxdist) + 
  ggtitle(paste0(&quot;id = &quot;, sel_id, &quot; - Number of points within distance = &quot;, 
                 length(id_inters)))
plot</code></pre>
<p><img src="/post/2018-02-20-speeding-up-spatial-analyses-by-integrating-sf-and-data-table-a-test-case_files/figure-html/unnamed-chunk-4-1.png" width="336" /></p>
<p><strong>So far, so good</strong>. Now, let’s do the same exercise with varying number of points
to see how it behaves in term of speed:</p>
<p><img src="/img/speeding_up/Rplot3.png" /></p>
<p>Already not bad! In particular for the maxdist = 2000 case, we get a quite large
speed improvement!</p>
<p>However, a nice thing about the points_in_distance approach is that <strong>it is easily
parallelizable</strong>. All is needed is to change some lines of the function <em>so that the
outer loop over the x “chunks” exploits a parallel backend</em> of some kind. (You can
find an example implementation exploiting foreach in <a href="https://gist.github.com/lbusett/247dc9b0b6bed04ac1b45c03999be348">this gist</a>)</p>
<p>On a not-particularly-fast PC, using a 6-cores parallelization leads to this:</p>
<p><img src="/img/speeding_up/Rplot4.png" /></p>
<p>Looking good! Some more skilled programmer could probably squeeze out even more
speed from it by some additional data.table magic, but the improvement is very
noticeable.</p>
<p>In terms of execution time, extrapolating again to the “infamous” 12 Million
points dataset, this would be what we get:</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
Method
</th>
<th style="text-align:right;">
Maxdist
</th>
<th style="text-align:left;">
Expected completion time (hours)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
st_intersect
</td>
<td style="text-align:right;">
500
</td>
<td style="text-align:left;">
<span style="     ">15</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
points_in_distance - serial
</td>
<td style="text-align:right;">
500
</td>
<td style="text-align:left;">
<span style="     ">2.5</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
points_in_distance - parallel
</td>
<td style="text-align:right;">
500
</td>
<td style="text-align:left;">
<span style="     ">0.57</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
st_intersect
</td>
<td style="text-align:right;">
2000
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: red !important;">85</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
points_in_distance - serial
</td>
<td style="text-align:right;">
2000
</td>
<td style="text-align:left;">
<span style="     ">15.2</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
points_in_distance - parallel
</td>
<td style="text-align:right;">
2000
</td>
<td style="text-align:left;">
<span style="     ">3.18</span>
</td>
</tr>
</tbody>
</table>
<p>So, we get a 5-6X speed improvement already on the “serial” implementation, and
another 5X thanks to parallelization over 6 cores! On themaxdist = 2000 case,
this means going <strong>from more than 3 days to about 3 hours</strong>. And if we had more cores
and RAM to throw at it, it would finish in minutes!</p>
<p>###<strong>Nice!</strong></p>
<p>##Final Notes</p>
<ul>
<li><p>The <strong>timings shown here are merely indicative</strong>, and related to the particular
test-dataset we built. On a less uniformly distributed dataset I would expect a
lower speed improvement.</p></li>
<li><p><strong>Some time is “wasted” because sf does not (yet) extend data.tables</strong>, making it
necessary to recreate sf objects from thedata.table subsets.</p></li>
<li><p>The parallel implementation is quick-and-dirty, and <strong>it is a bit of a memory-hog</strong>!
Be careful before throwing at it 25 processors!</p></li>
<li><p><strong>Speed is influenced in a non-trivial way by the number of “cuts”</strong> used to
subdivide the spatial extent. There may be a sweet-spot related to points
distribution and maxdist allowing reaching maximum speed.</p></li>
<li><p>A similar approach for parallelization could exploit repeatedly “cropping” the
original sf points object over the extent of the chunk/extended chunk. The
<strong>data.table approach seems however to be faster</strong>.</p></li>
</ul>
<p><strong>That’s all! Hope you liked this (rather long) post!</strong></p>
</div>
