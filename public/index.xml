<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Lorenzo Busetto Website &amp; Blog on Lorenzo Busetto Website &amp; Blog</title>
    <link>/</link>
    <description>Recent content in Lorenzo Busetto Website &amp; Blog on Lorenzo Busetto Website &amp; Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>MODIStsp v. 1.4.0 is out !</title>
      <link>/post/modistsp-v-1-4-0-is-out/</link>
      <pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/modistsp-v-1-4-0-is-out/</guid>
      <description>&lt;p&gt;A new version of &lt;a href=&#34;http://ropensci.github.io/MODIStsp/&#34;&gt;MODIStsp&lt;/a&gt; (1.4.0) is on &lt;a href=&#34;https://cran.r-project.org/web/packages/MODIStsp/index.html&#34;&gt;CRAN&lt;/a&gt; as of today !&lt;/p&gt;
&lt;p&gt;This version:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Switches to use of GDAL3/PROJ6 WKTs for projection representation, using sf::gdal_utils to perform gdalwarp/gdaltranslate instead of gdalUtils on external GDAL;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Switches to use of &lt;code&gt;sf&lt;/code&gt; for all internal work on vector data;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Removes sp, rgdal, rgeos, pacman, gdalUtils dependencies;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Adds support for products MCD19A1 and MCD19A2 products.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      </item>
    
    <item>
      <title>sen2r: An R toolbox for automatically downloading and preprocessing Sentinel-2 satellite data</title>
      <link>/publication/2020-01-01_sen2r/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/publication/2020-01-01_sen2r/</guid>
      <description></description>
      </item>
    
    <item>
      <title>MODIStsp v. 1.3.9 is out !</title>
      <link>/post/modistsp-v-1-3-9-is-out/</link>
      <pubDate>Mon, 24 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/modistsp-v-1-3-9-is-out/</guid>
      <description>&lt;p&gt;A new version of &lt;a href=&#34;http://ropensci.github.io/MODIStsp/&#34;&gt;MODIStsp&lt;/a&gt; (1.3.9) is on &lt;a href=&#34;https://cran.r-project.org/web/packages/MODIStsp/index.html&#34;&gt;CRAN&lt;/a&gt; as of today !&lt;/p&gt;
&lt;p&gt;This version:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Introduces support for the new MCD12Q2 v006 product and removes support for some v005 product no longer available (&lt;a href=&#34;https://github.com/ropensci/MODIStsp/issues/170&#34; class=&#34;uri&#34;&gt;https://github.com/ropensci/MODIStsp/issues/170&lt;/a&gt;);&lt;/li&gt;
&lt;li&gt;Fixes a bug preventing correct processing of product MOD14A1 (&lt;a href=&#34;https://github.com/ropensci/MODIStsp/issues/170&#34; class=&#34;uri&#34;&gt;https://github.com/ropensci/MODIStsp/issues/170&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Updates all links to the MODIS products’ description web pages&lt;/li&gt;
&lt;/ul&gt;
</description>
      </item>
    
    <item>
      <title>MODIStsp v. 1.3.8 is out !</title>
      <link>/post/modistsp-v-1-3-8-is-out/</link>
      <pubDate>Thu, 07 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/modistsp-v-1-3-8-is-out/</guid>
      <description>&lt;p&gt;A new version of &lt;a href=&#34;http://ropensci.github.io/MODIStsp/&#34;&gt;MODIStsp&lt;/a&gt; (1.3.8) is on &lt;a href=&#34;https://cran.r-project.org/web/packages/MODIStsp/index.html&#34;&gt;CRAN&lt;/a&gt; as of today !&lt;/p&gt;
&lt;p&gt;The new version fixes a nasty issue introduced by changes in &lt;code&gt;gdal_buildvrt&lt;/code&gt; behaviour in GDAL &amp;gt; 2.3,
(&lt;a href=&#34;https://trac.osgeo.org/gdal/ticket/3221#comment:5&#34; class=&#34;uri&#34;&gt;https://trac.osgeo.org/gdal/ticket/3221#comment:5&lt;/a&gt;) which caused problems in proper application of scales and offset on MODIS layers - see &lt;a href=&#34;https://github.com/ropensci/MODIStsp/issues/163&#34; class=&#34;uri&#34;&gt;https://github.com/ropensci/MODIStsp/issues/163&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you are experiencing problems with &lt;code&gt;MODIStsp&lt;/code&gt; and you have GDAL &amp;gt; 2.3 on your system, you
are strongly encouraged to update the package!&lt;/p&gt;
&lt;p&gt;Besides this, &lt;code&gt;MODIStsp 1.3.8&lt;/code&gt; introduces some minor bug fixes, and &lt;strong&gt;adds support for
products MOD21A1D.006, MOD21A1N.006 and MOD21A2.006&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The full changelog for v1.3.8 can be found &lt;a href=&#34;https://github.com/ropensci/MODIStsp/releases/tag/v1.3.8&#34;&gt;HERE&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We hope you will find the new version useful and that we didn’t introduce too
many bugs !&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;As usual, please report any problems in our &lt;a href=&#34;https://github.com/ropensci/MODIStsp/issues&#34;&gt;issues&lt;/a&gt; GitHub page.&lt;/strong&gt;&lt;/p&gt;
</description>
      </item>
    
    <item>
      <title>A new spatial modeling and interpolation approach for high-resolution temperature maps combining reanalysis data and ground measurements</title>
      <link>/publication/2019-01-01-ag_met_viggiano/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/2019-01-01-ag_met_viggiano/</guid>
      <description></description>
      </item>
    
    <item>
      <title>Bug discovered in MODIStsp!</title>
      <link>/post/modistsp-bug/</link>
      <pubDate>Wed, 08 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/modistsp-bug/</guid>
      <description>&lt;p&gt;We are sorry to report that we recently discovered a nasty bug (or rather, a stupid mistake…) in the &lt;a href=&#34;https://github.com/ropensci/MODIStsp&#34;&gt;MODIStsp&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;The bug led to improper computation of custom spectral indices in the case that their formula included addition or subtraction operations on reflectance values (e.g., something like &lt;span class=&#34;math inline&#34;&gt;\(\frac{(\rho_{NIR}+0.1)}{\rho_{Red}}\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; indicating a reflectance).&lt;/p&gt;
&lt;div id=&#34;what-is-affected&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is affected&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Values of the following &lt;em&gt;Additional Spectral Indices&lt;/em&gt; selectable using the MODIStsp GUI:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;EVI&lt;/li&gt;
&lt;li&gt;SAVI&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;, &lt;em&gt;in the case that the &lt;strong&gt;Apply Scale/Offset&lt;/strong&gt; option was set to “No”&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Values of any &lt;em&gt;custom spectral indexes&lt;/em&gt; added by the user, in case they included additive or subtractive coefficients.&lt;/p&gt;
&lt;p&gt;, &lt;em&gt;in the case that the &lt;strong&gt;Apply Scale/Offset&lt;/strong&gt; option was set to “No”&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-not-affected&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is NOT affected&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Values of spectral indexes available in MODIS HDF images as original sds layers (e.g., EVI in MOD13Q1)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Values of any additional / custom spectral indexes in case they did not include additive or
subtractive coefficients, or the &lt;strong&gt;Apply Scale/Offset&lt;/strong&gt; option was set to “Yes”&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;what-to-do-if-you-are-affected&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What to do if you are affected&lt;/h2&gt;
&lt;p&gt;The bug is now fixed on the GitHub version. A patched release will be made available on CRAN as soon as possible.&lt;/p&gt;
&lt;p&gt;Unfortunately, if you have
time series processed with the old version falling in the “What is affected” category, there’s nothing you can do, save for reprocessing them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We are truly sorry for the problem&lt;/strong&gt;, which somehow managed to slip under the radar until now.
We hope it will not bring you too much trouble!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-exactly-was-the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What exactly was the problem?&lt;/h2&gt;
&lt;p&gt;This is &lt;strong&gt;so basic that can easily go unnoticed.&lt;/strong&gt; So it’s better to document it…&lt;/p&gt;
&lt;p&gt;MODIS reflectances are stored in HDF layers as integers with a 10000 scale factor (e.g., a 0.1 reflectance is stored as 1000). If you need to “manually” compute an index such as SAVI:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(SAVI = \frac{(\rho_{NIR} - \rho_{Red})}{(\rho_{NIR} + \rho_{Red} + 0.5)} * (1 + 0.5)\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;starting from MODIS reflectances, you must take care of multiplying the MODIS data by 10E-4 beforehand. Your formula then becomes:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(SAVI = \frac{(0.0001 * b2_{NIR} - 0.0001 * b1_{Red})}{0.0001 * b2_{NIR} + 0.0001 * b1_{Red} + 0.5} * (1 + 0.5)\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;, otherwise the additive constants (in this case, the &lt;span class=&#34;math inline&#34;&gt;\(+ 0.5\)&lt;/span&gt; in the denominator) would be made practically irrelevant.&lt;/p&gt;
&lt;/div&gt;
</description>
      </item>
    
    <item>
      <title>MODIStsp approved on rOpenSci!</title>
      <link>/post/modistsp-approved-on-ropensci/</link>
      <pubDate>Wed, 08 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/modistsp-approved-on-ropensci/</guid>
      <description>&lt;p&gt;We are happy to report that our &lt;a href=&#34;https://github.com/ropensci/MODIStsp&#34;&gt;MODIStsp&lt;/a&gt; package for
automatic preprocessing of &lt;strong&gt;MODIS time series&lt;/strong&gt; has been recently approved for being included in the &lt;a href=&#34;https://ropensci.org/packages/&#34;&gt;rOpenSci&lt;/a&gt; ecosystem of &lt;code&gt;R&lt;/code&gt; packages for reproducible science!&lt;/p&gt;
&lt;p&gt;We wish to thank reviewers &lt;a href=&#34;https://github.com/lwasser&#34;&gt;&lt;strong&gt;Leah Wasser&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/jeffreyhanson&#34;&gt;&lt;strong&gt;Jeffrey Hanson&lt;/strong&gt;&lt;/a&gt; for providing really valuable insights during
the &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/184&#34;&gt;onboarding review process&lt;/a&gt;. We think their
contribution really helped in improving the package!&lt;/p&gt;
&lt;p&gt;Please also note that &lt;strong&gt;MODIStsp website was also migrated&lt;/strong&gt;, and is now available
at &lt;a href=&#34;http://ropensci.github.io/MODIStsp/&#34; class=&#34;uri&#34;&gt;http://ropensci.github.io/MODIStsp/&lt;/a&gt;&lt;/p&gt;
</description>
      </item>
    
    <item>
      <title>MODIStsp v. 1.3.4 is out ! Now allowing interactive definition of processing extent!</title>
      <link>/post/modistsp-v-1-3-4-is-out-now-allowing-interactive-definition-of-processing-extent/</link>
      <pubDate>Mon, 11 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/modistsp-v-1-3-4-is-out-now-allowing-interactive-definition-of-processing-extent/</guid>
      <description>&lt;p&gt;We are happy to report that a new version of &lt;a href=&#34;http://ropensci.github.io/MODIStsp/&#34;&gt;MODIStsp&lt;/a&gt; (1.3.4) is on &lt;a href=&#34;https://cran.r-project.org/web/packages/MODIStsp/index.html&#34;&gt;CRAN&lt;/a&gt; as of today !&lt;/p&gt;
&lt;p&gt;The new version introduces a &lt;strong&gt;strongly improved GUI&lt;/strong&gt; (thanks mainly to &lt;span class=&#34;citation&#34;&gt;@lwasser&lt;/span&gt;
comments in her &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/184&#34;&gt;review for MODIStsp onboarding on ropensci&lt;/a&gt;). The new GUI facilitates
the selection of layers to be processed, and allows interactive selection of
the processing spatial extent over a map (thanks to &lt;span class=&#34;citation&#34;&gt;@timsalabim&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;@timelyportfolio&lt;/span&gt;
for implementing some changes on mapview to allow this!). The main changes introduced in the GUI are
&lt;strong&gt;highlighted in the image and animation below&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-11-modistsp-v-1-3-4-is-out-now-allowing-interactive-definition-of-processing-extent_files/MODIStsp_newgui.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-11-modistsp-v-1-3-4-is-out-now-allowing-interactive-definition-of-processing-extent_files/animation_1.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Besides this, the &lt;strong&gt;main changes with respect to version 1.3.3&lt;/strong&gt; are related to:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Inclusion of some &lt;strong&gt;new MODIS products&lt;/strong&gt;;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Removal of almost all v005 products&lt;/strong&gt;, which were recently deprecated by NASA;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Removal of FTP download&lt;/strong&gt; functionality again due to deprecation from NASA;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Improvement in functionality for dealing with NoData&lt;/strong&gt; for products with multiple
fill-values. If “Change NoData” is set to “Yes”, then in case a layer
has multiple Nodata values all those values are set to NA in the output;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Miscellaneous bug fixing&lt;/strong&gt; and code refactoring.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The full changelog for v1.3.4 can be found &lt;a href=&#34;https://github.com/ropensci/MODIStsp/releases/tag/v1.3.4&#34;&gt;HERE&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We hope you will find the new version useful and that we didn’t introduce too
many bugs ! &lt;strong&gt;Please report any problems in our
&lt;a href=&#34;https://github.com/ropensci/MODIStsp/issues&#34;&gt;issues&lt;/a&gt; GitHub page.&lt;/strong&gt;&lt;/p&gt;
</description>
      </item>
    
    <item>
      <title>A new RStudio addin to facilitate inserting tables in Rmarkdown documents</title>
      <link>/post/a-new-rstudio-addin-to-facilitate-inserting-tables-in-rmarkdown-documents/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/a-new-rstudio-addin-to-facilitate-inserting-tables-in-rmarkdown-documents/</guid>
      <description>&lt;p&gt;In the last months, I started increasingly using &lt;code&gt;Rmd&lt;/code&gt; documents for preparing scientific reports, blog posts, etcetera. While I really like the flexibility offered by the system, one thing that I thought could be improved is the support for &lt;strong&gt;easily inserting tables&lt;/strong&gt;. So, “inspired” also by the recent addition of the excellent &lt;a href=&#34;http://lcolladotor.github.io/2018/03/07/blogdown-insert-image-addin/#.WsEt2YhuaHs&#34;&gt;&lt;strong&gt;&lt;code&gt;insert image&lt;/code&gt; addin&lt;/strong&gt;&lt;/a&gt; in &lt;code&gt;blogdown&lt;/code&gt;, I decided to give it a go and try to implement some kind of addin to facilitate table insertion in &lt;code&gt;Rmd&lt;/code&gt; documents.&lt;/p&gt;
&lt;p&gt;After struggling a bit due to my rather nonexistent shiny skills, in the end I managed to obtain a “basic but useful” (IMO) addin. Let’s see how it works:&lt;/p&gt;
&lt;div id=&#34;installation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;p&gt;You can install the addin from &lt;a href=&#34;https://github.com/lbusett/insert_table&#34;&gt;GitHub&lt;/a&gt; with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install.packages(&amp;quot;devtools&amp;quot;)
devtools::install_github(&amp;quot;lbusett/insert_table&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;usage&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;p&gt;Upon installing, &lt;code&gt;inserttable&lt;/code&gt; registers a new RStudio Addin (&lt;strong&gt;Insert Table&lt;/strong&gt;)
that can be used to easily insert a table in a &lt;code&gt;Rmd&lt;/code&gt; document. To use it, open a &lt;code&gt;Rmd&lt;/code&gt; document and, with the cursor within a &lt;code&gt;r&lt;/code&gt; chunk and select “Addins –&amp;gt; Insert Table”.&lt;/p&gt;
&lt;p&gt;There are two main &lt;strong&gt;use modes&lt;/strong&gt;:&lt;/p&gt;
&lt;div id=&#34;launch-the-addin-while-the-cursor-is-on-a-empty-line&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Launch the addin while the cursor is on a empty line&lt;/h3&gt;
&lt;p&gt;In this case, a GUI will open allowing you to &lt;strong&gt;select the desired output format&lt;/strong&gt; (
&lt;code&gt;kableExtra&lt;/code&gt;, &lt;code&gt;DT&lt;/code&gt; and &lt;code&gt;rhandsontable&lt;/code&gt; are currently implemented), and to &lt;strong&gt;edit the content of the table&lt;/strong&gt;. After clicking &lt;strong&gt;Done&lt;/strong&gt; the Addin will add in the &lt;code&gt;Rmd&lt;/code&gt;
document the code needed to generate the table in a nice &lt;code&gt;tribble&lt;/code&gt; format (thanks to Miles McBain’s &lt;a href=&#34;https://github.com/milesmcbain/datapasta&#34;&gt;&lt;code&gt;datapasta&lt;/code&gt;&lt;/a&gt; package!) to allow easier editing, and also the code needed to render it with the selected output format using some
default options, as can be seen below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/inserttable/animation_1.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A useful feature is that, for larger tables, you can also &lt;strong&gt;cut and paste content from a spreadsheet&lt;/strong&gt; :&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/inserttable/animation_2.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Obviously, rendering of the table can be tweaked further by changing/adding arguments of the rendering functions in the automatically generated code.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;launch-the-addin-while-selecting-the-name-of-a-variable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Launch the addin while selecting the name of a variable&lt;/h3&gt;
&lt;p&gt;In this case, the GUI allows you to select &lt;strong&gt;only the desired output format&lt;/strong&gt; (
it is assumed that the variable you select corresponds to a &lt;code&gt;data frame&lt;/code&gt; or similar
object containing the data you wish to show as table). After clicking &lt;strong&gt;Done&lt;/strong&gt;
the Addin will add in the &lt;code&gt;Rmd&lt;/code&gt; document the code needed to render the selected variable as a table with the selected output format. The code will be added at the first empty line below that containing the name of the selected variable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/inserttable/animation_3.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;IMPORTANT NOTE&lt;/strong&gt;: &lt;code&gt;inserttable&lt;/code&gt; will make no effort to guarantee that the
variable you select is a &lt;code&gt;data.frame&lt;/code&gt;. It is up to you to select a meaningful
variable!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;usage-from-the-console&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Usage from the console&lt;/h2&gt;
&lt;p&gt;You can also use (part of) &lt;code&gt;inserttable&lt;/code&gt; functionality from the console by calling
function &lt;code&gt;insert_table()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; insert_table(tbl_name = &amp;quot;table_1&amp;quot;, nrows = 4, ncols = 4, tbl_format = &amp;quot;DT&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function will return &lt;strong&gt;to the console&lt;/strong&gt; the code needed to create a empty
table of the specified dimensions and render it with the selected format:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/inserttable/animation_4.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;####&lt;strong&gt;That’s all&lt;/strong&gt;!.&lt;/p&gt;
&lt;p&gt;I hope someone else will find this useful!&lt;/p&gt;
&lt;/div&gt;
</description>
      </item>
    
    <item>
      <title>Automatically importing publications from bibtex to a hugo-academic blog</title>
      <link>/post/automatically-importing-publications-from-bibtex-to-a-hugo-academic-blog/</link>
      <pubDate>Tue, 13 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/automatically-importing-publications-from-bibtex-to-a-hugo-academic-blog/</guid>
      <description>&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The problem&lt;/h2&gt;
&lt;p&gt;I am in the process of migrating my (rather ugly) &lt;a href=&#34;https://www.blogger.com/blogger.g?blogID=4683863749820351779#allposts&#34;&gt;small blog&lt;/a&gt; from
“Bloggers” to blogdown and, as several others, I choose to use the &lt;a href=&#34;https://github.com/gcushen/hugo-academic&#34;&gt;hugo-academic theme&lt;/a&gt; due to its good looks,
simplicity, and “focus” towards researchers.&lt;/p&gt;
&lt;p&gt;One nice feature of &lt;code&gt;hugo-academic&lt;/code&gt; is that it includes out-of-the-box a “Publications”
section, allowing researchers to easily create a list of their publication as a
section of the website.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unfortunately&lt;/strong&gt;, in order to populate that list, users have to manually create one
different &lt;code&gt;.md&lt;/code&gt; file for each publication, by cutting and pasting several different
info (e.g., title, authors, etc.) in a “simple”&amp;quot; template like &lt;a href=&#34;https://github.com/gcushen/hugo-academic/blob/master/archetypes/publication.md&#34;&gt;this one&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;THIS IS BORING!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Since I was not in the mood of doing that, and no automatic solutions could be
found (well, there appears to be a &lt;code&gt;python&lt;/code&gt; one, but we are speaking &lt;code&gt;R&lt;/code&gt;, here…), I
decided to try and develop some script to automatically create the required &lt;code&gt;md&lt;/code&gt;
files starting from a &lt;code&gt;BibTex&lt;/code&gt; list of my publications. Here are the results of
that effort.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-possible-solution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A possible solution&lt;/h2&gt;
&lt;div id=&#34;preparing-the-bibtex-file&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Preparing the &lt;code&gt;BibTex&lt;/code&gt; file&lt;/h3&gt;
&lt;p&gt;To automatically create the publications &lt;code&gt;md&lt;/code&gt; files, all you need is a (properly formatted)
&lt;code&gt;BibTex&lt;/code&gt; file. Since I did not have one ready, I created mine by exporting my
publications list from &lt;a href=&#34;https://www.scopus.com/authid/detail.uri?authorId=23003461400&#34;&gt;Scopus&lt;/a&gt;, but you
could use any valid BibTeX file.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;One important thing, though, is that you have to be sure that the file is saved
with UTF-8 encoding&lt;/strong&gt;. If you are not sure, you can open it in RStudio (or any decent
text editor), and then re-save it with a forced encoding (in RStudio, you can use
&lt;code&gt;File--&amp;gt;Save with Encoding&lt;/code&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-an-import-script&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating an import script&lt;/h3&gt;
&lt;p&gt;Now, you need a script that reads the &lt;code&gt;BibTex&lt;/code&gt; entries and use the data to populate
one different &lt;code&gt;md&lt;/code&gt; file for each publication. Below you can find my attempt at this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bibtex_2academic &amp;lt;- function(bibfile,
                             outfold,
                             abstract = FALSE, 
                             overwrite = FALSE) {
  
  require(RefManageR)
  require(dplyr)
  require(stringr)
  require(anytime)
  
  # Import the bibtex file and convert to data.frame
  mypubs   &amp;lt;- ReadBib(bibfile, check = &amp;quot;warn&amp;quot;, .Encoding = &amp;quot;UTF-8&amp;quot;) %&amp;gt;%
    as.data.frame()
  
  # assign &amp;quot;categories&amp;quot; to the different types of publications
  mypubs   &amp;lt;- mypubs %&amp;gt;%
    dplyr::mutate(
      pubtype = dplyr::case_when(document_type == &amp;quot;Article&amp;quot; ~ &amp;quot;2&amp;quot;,
                                 document_type == &amp;quot;Article in Press&amp;quot; ~ &amp;quot;2&amp;quot;,
                                 document_type == &amp;quot;InProceedings&amp;quot; ~ &amp;quot;1&amp;quot;,
                                 document_type == &amp;quot;Proceedings&amp;quot; ~ &amp;quot;1&amp;quot;,
                                 document_type == &amp;quot;Conference&amp;quot; ~ &amp;quot;1&amp;quot;,
                                 document_type == &amp;quot;Conference Paper&amp;quot; ~ &amp;quot;1&amp;quot;,
                                 document_type == &amp;quot;MastersThesis&amp;quot; ~ &amp;quot;3&amp;quot;,
                                 document_type == &amp;quot;PhdThesis&amp;quot; ~ &amp;quot;3&amp;quot;,
                                 document_type == &amp;quot;Manual&amp;quot; ~ &amp;quot;4&amp;quot;,
                                 document_type == &amp;quot;TechReport&amp;quot; ~ &amp;quot;4&amp;quot;,
                                 document_type == &amp;quot;Book&amp;quot; ~ &amp;quot;5&amp;quot;,
                                 document_type == &amp;quot;InCollection&amp;quot; ~ &amp;quot;6&amp;quot;,
                                 document_type == &amp;quot;InBook&amp;quot; ~ &amp;quot;6&amp;quot;,
                                 document_type == &amp;quot;Misc&amp;quot; ~ &amp;quot;0&amp;quot;,
                                 TRUE ~ &amp;quot;0&amp;quot;))
  
  # create a function which populates the md template based on the info
  # about a publication
  create_md &amp;lt;- function(x) {
    
    # define a date and create filename by appending date and start of title
    if (!is.na(x[[&amp;quot;year&amp;quot;]])) {
      x[[&amp;quot;date&amp;quot;]] &amp;lt;- paste0(x[[&amp;quot;year&amp;quot;]], &amp;quot;-01-01&amp;quot;)
    } else {
      x[[&amp;quot;date&amp;quot;]] &amp;lt;- &amp;quot;2999-01-01&amp;quot;
    }
    
    filename &amp;lt;- paste(x[[&amp;quot;date&amp;quot;]], x[[&amp;quot;title&amp;quot;]] %&amp;gt;%
                        str_replace_all(fixed(&amp;quot; &amp;quot;), &amp;quot;_&amp;quot;) %&amp;gt;%
                        str_remove_all(fixed(&amp;quot;:&amp;quot;)) %&amp;gt;%
                        str_sub(1, 20) %&amp;gt;%
                        paste0(&amp;quot;.md&amp;quot;), sep = &amp;quot;_&amp;quot;)
    # start writing
    if (!file.exists(file.path(outfold, filename)) | overwrite) {
      fileConn &amp;lt;- file.path(outfold, filename)
      write(&amp;quot;+++&amp;quot;, fileConn)
      
      # Title and date
      write(paste0(&amp;quot;title = \&amp;quot;&amp;quot;, x[[&amp;quot;title&amp;quot;]], &amp;quot;\&amp;quot;&amp;quot;), fileConn, append = T)
      write(paste0(&amp;quot;date = \&amp;quot;&amp;quot;, anydate(x[[&amp;quot;date&amp;quot;]]), &amp;quot;\&amp;quot;&amp;quot;), fileConn, append = T)
      
      # Authors. Comma separated list, e.g. `[&amp;quot;Bob Smith&amp;quot;, &amp;quot;David Jones&amp;quot;]`.
      auth_hugo &amp;lt;- str_replace_all(x[&amp;quot;author&amp;quot;], &amp;quot; and &amp;quot;, &amp;quot;\&amp;quot;, \&amp;quot;&amp;quot;)
      auth_hugo &amp;lt;- stringi::stri_trans_general(auth_hugo, &amp;quot;latin-ascii&amp;quot;)
      write(paste0(&amp;quot;authors = [\&amp;quot;&amp;quot;, auth_hugo,&amp;quot;\&amp;quot;]&amp;quot;), fileConn, append = T)
      
      # Publication type. Legend:
      # 0 = Uncategorized, 1 = Conference paper, 2 = Journal article
      # 3 = Manuscript, 4 = Report, 5 = Book,  6 = Book section
      write(paste0(&amp;quot;publication_types = [\&amp;quot;&amp;quot;, x[[&amp;quot;pubtype&amp;quot;]],&amp;quot;\&amp;quot;]&amp;quot;), 
            fileConn, append = T)
      
      # Publication details: journal, volume, issue, page numbers and doi link
      publication &amp;lt;- x[[&amp;quot;journal&amp;quot;]]
      if (!is.na(x[[&amp;quot;volume&amp;quot;]])) publication &amp;lt;- paste0(publication, 
                                                       &amp;quot;, (&amp;quot;, x[[&amp;quot;volume&amp;quot;]], &amp;quot;)&amp;quot;)
      if (!is.na(x[[&amp;quot;number&amp;quot;]])) publication &amp;lt;- paste0(publication,
                                                       &amp;quot;, &amp;quot;, x[[&amp;quot;number&amp;quot;]])
      if (!is.na(x[[&amp;quot;pages&amp;quot;]])) publication &amp;lt;- paste0(publication,
                                                      &amp;quot;, _pp. &amp;quot;, x[[&amp;quot;pages&amp;quot;]], &amp;quot;_&amp;quot;)
      if (!is.na(x[[&amp;quot;doi&amp;quot;]])) publication &amp;lt;- paste0(publication,
                                                    &amp;quot;, &amp;quot;, paste0(&amp;quot;https://doi.org/&amp;quot;, 
                                                                 x[[&amp;quot;doi&amp;quot;]]))
      
      write(paste0(&amp;quot;publication = \&amp;quot;&amp;quot;, publication,&amp;quot;\&amp;quot;&amp;quot;), fileConn, append = T)
      write(paste0(&amp;quot;publication_short = \&amp;quot;&amp;quot;, publication,&amp;quot;\&amp;quot;&amp;quot;),fileConn, append = T)
      
      # Abstract and optional shortened version.
      if (abstract) {
        write(paste0(&amp;quot;abstract = \&amp;quot;&amp;quot;, x[[&amp;quot;abstract&amp;quot;]],&amp;quot;\&amp;quot;&amp;quot;), fileConn, append = T)
      } else {
        write(&amp;quot;abstract = \&amp;quot;\&amp;quot;&amp;quot;, fileConn, append = T)
      }
      write(paste0(&amp;quot;abstract_short = \&amp;quot;&amp;quot;,&amp;quot;\&amp;quot;&amp;quot;), fileConn, append = T)
      
      # other possible fields are kept empty. They can be customized later by 
      # editing the created md
      
      write(&amp;quot;image_preview = \&amp;quot;\&amp;quot;&amp;quot;, fileConn, append = T)
      write(&amp;quot;selected = false&amp;quot;, fileConn, append = T)
      write(&amp;quot;projects = []&amp;quot;, fileConn, append = T)
      write(&amp;quot;tags = []&amp;quot;, fileConn, append = T)
      #links
      write(&amp;quot;url_pdf = \&amp;quot;\&amp;quot;&amp;quot;, fileConn, append = T)
      write(&amp;quot;url_preprint = \&amp;quot;\&amp;quot;&amp;quot;, fileConn, append = T)
      write(&amp;quot;url_code = \&amp;quot;\&amp;quot;&amp;quot;, fileConn, append = T)
      write(&amp;quot;url_dataset = \&amp;quot;\&amp;quot;&amp;quot;, fileConn, append = T)
      write(&amp;quot;url_project = \&amp;quot;\&amp;quot;&amp;quot;, fileConn, append = T)
      write(&amp;quot;url_slides = \&amp;quot;\&amp;quot;&amp;quot;, fileConn, append = T)
      write(&amp;quot;url_video = \&amp;quot;\&amp;quot;&amp;quot;, fileConn, append = T)
      write(&amp;quot;url_poster = \&amp;quot;\&amp;quot;&amp;quot;, fileConn, append = T)
      write(&amp;quot;url_source = \&amp;quot;\&amp;quot;&amp;quot;, fileConn, append = T)
      #other stuff
      write(&amp;quot;math = true&amp;quot;, fileConn, append = T)
      write(&amp;quot;highlight = true&amp;quot;, fileConn, append = T)
      # Featured image
      write(&amp;quot;[header]&amp;quot;, fileConn, append = T)
      write(&amp;quot;image = \&amp;quot;\&amp;quot;&amp;quot;, fileConn, append = T)
      write(&amp;quot;caption = \&amp;quot;\&amp;quot;&amp;quot;, fileConn, append = T)
      
      write(&amp;quot;+++&amp;quot;, fileConn, append = T)
    }
  }
  # apply the &amp;quot;create_md&amp;quot; function over the publications list to generate
  # the different &amp;quot;md&amp;quot; files.
  
  apply(mypubs, FUN = function(x) create_md(x), MARGIN = 1)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nothing fancy, here: I just use the &lt;a href=&#34;https://cran.r-project.org/web/packages/RefManageR/index.html&#34;&gt;&lt;code&gt;RefManageR&lt;/code&gt;&lt;/a&gt;
package to read the BibTeX file, and then cycle over publications to create
files properly formatted for &lt;code&gt;hugo-academic&lt;/code&gt; use.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-the-script&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Running the script&lt;/h3&gt;
&lt;p&gt;All is left is to run the script:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_bibfile &amp;lt;- &amp;quot;/path/to/mybibtex.bib&amp;quot;
out_fold   &amp;lt;- &amp;quot;/path/to/myoutfolder&amp;quot;

bibtex_2academic(bibffile  = my_bibfile, 
                 outfold   = out_fold, 
                 abstract  = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;outfold&lt;/code&gt; argument allows specifying where the generated &lt;code&gt;md&lt;/code&gt; files will be
saved. Though in the end they will have to be moved to folder &lt;code&gt;content/publication&lt;/code&gt; you
may want to save them at first in a different folder to be able to check them
before trying to deploy.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;abstract&lt;/code&gt; argument specifies whether to include the abstract in the &lt;code&gt;md&lt;/code&gt; or
not.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Running the script will give you files like this one:&lt;/p&gt;
&lt;pre class=&#34;yml&#34;&gt;&lt;code&gt;+++
title = &amp;quot;Estimating canopy water content of poplar plantation from MIVIS data&amp;quot;
date = &amp;quot;2006-01-01&amp;quot;
authors = [&amp;quot;R. Colombo&amp;quot;, &amp;quot;L. Busetto&amp;quot;, &amp;quot;A. Marchesi&amp;quot;, &amp;quot;M. Meroni&amp;quot;, &amp;quot;C. Giardino&amp;quot;]
publication_types = [&amp;quot;1&amp;quot;]
publication = &amp;quot;AIP Conference Proceedings, (852), _pp. 242-249_, https://doi.org/10.1063/1.2349350&amp;quot;
publication_short = &amp;quot;&amp;quot;
abstract_short = &amp;quot;&amp;quot;
image_preview = &amp;quot;&amp;quot;
selected = false
projects = []
tags = []
url_pdf = &amp;quot;&amp;quot;
url_preprint = &amp;quot;&amp;quot;
url_code = &amp;quot;&amp;quot;
url_dataset = &amp;quot;&amp;quot;
url_project = &amp;quot;&amp;quot;
url_slides = &amp;quot;&amp;quot;
url_video = &amp;quot;&amp;quot;
url_poster = &amp;quot;&amp;quot;
url_source = &amp;quot;&amp;quot;
math = false
highlight = true
[header]
image = &amp;quot;&amp;quot;
caption = &amp;quot;&amp;quot;
+++&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;, where I tweaked a bit the hugo-academic format to include bibliographic
info such as volume, number, pages and doi link. The files can then be further customized
to include, for example, links to pdfs, images, etcetera.&lt;/p&gt;
&lt;p&gt;After moving all the &lt;code&gt;md&lt;/code&gt; files to &lt;code&gt;content/publication&lt;/code&gt;, the publications section
of your &lt;code&gt;hugo-academic&lt;/code&gt; site will be auto-populated, and should look more or less
like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/publications/publications.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can have a look at the final results on my (under construction) website &lt;a href=&#34;https://lbusett.netlify.com/publication/&#34;&gt;here&lt;/a&gt;. I think it’s quite nice!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;final-notes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Final Notes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;My importing script is quite “quick and dirty”. It &lt;strong&gt;does not attempt to deal
with special characters, and even substitutes accented letters with “bare”
letters to avoid rendering problems&lt;/strong&gt;. If someone more knowledgeable about encoding
issues wants to try and improve it, I put it in &lt;a href=&#34;https://gist.github.com/lbusett/da7b1fba4345e03192a450226a17636e&#34;&gt;this gist&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It may happen that your site will stop rendering when you put the new &lt;code&gt;md&lt;/code&gt; files
in &lt;code&gt;content/publication&lt;/code&gt;. If so, the reason is probably that you have some “strange”
formatting in some of the files. Usual suspects would be unproperly recognized &lt;strong&gt;accents&lt;/strong&gt;,
&lt;strong&gt;math formulas&lt;/strong&gt; or other special characters in the &lt;strong&gt;authors&lt;/strong&gt; and &lt;strong&gt;abstract&lt;/strong&gt;
fields. You will have to look into each file and remove any offending areas.
(It happened to me a lot before properly saving to UTF-8)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
      </item>
    
    <item>
      <title>Speeding up spatial analyses by integrating `sf` and `data.table`: a test case</title>
      <link>/post/speeding-up-spatial-analyses-by-integrating-sf-and-data-table-a-test-case/</link>
      <pubDate>Tue, 20 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/speeding-up-spatial-analyses-by-integrating-sf-and-data-table-a-test-case/</guid>
      <description>&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The problem&lt;/h2&gt;
&lt;p&gt;Last week, I replied to &lt;a href=&#34;https://stackoverflow.com/questions/48650274/spatial-efficient-way-of-finding-all-points-within-x-meters-of-a-point&#34;&gt;this interesting question&lt;/a&gt; posted by &lt;span class=&#34;citation&#34;&gt;@Tim_K&lt;/span&gt; over stackoverflow. He was seeking
efficient solutions to identify all points falling within a maximum distance of
xx meters with respect to each single point in a spatial points dataset.&lt;/p&gt;
&lt;p&gt;If you have a look at the thread, you will see that a simple solution based on
creating a “buffered” polygon dataset beforehand and then intersecting it with
the original points is quite fast for “reasonably sized” datasets, thanks to sf
spatial indexing capabilities which reduce the number of the required comparisons
to be done (See &lt;a href=&#34;http://r-spatial.org/r/2017/06/22/spatial-index.html&#34; class=&#34;uri&#34;&gt;http://r-spatial.org/r/2017/06/22/spatial-index.html&lt;/a&gt;). In practice,
something like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create test data: 50000 uniformly distributed points on a &amp;quot;square&amp;quot; of 100000
# metres
maxdist &amp;lt;- 500
pts     &amp;lt;- data.frame(x = runif(50000, 0, 100000),
                      y = runif(50000, 0, 100000),
                      id = 1:50000) %&amp;gt;%
  sf::st_as_sf(coords = c(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;))
# create buffered polygons
pts_buf &amp;lt;- sf::st_buffer(pts, maxdist)
# Find points within 500 meters wrt each point
int &amp;lt;- sf::st_intersects(pts_buf, pts)
int&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Sparse geometry binary predicate list of length 50000, where the predicate was `intersects&amp;#39;
## first 10 elements:
##  1: 1, 39364, 43452
##  2: 2, 24400, 26773, 27460
##  3: 3, 26700, 32063, 38651, 40326
##  4: 4, 5351, 6136, 12632, 25758, 29705
##  5: 5, 6423, 7148, 40104
##  6: 6, 677, 1603, 10881, 14026, 16526, 25497, 29151
##  7: 7, 3291, 6757, 23374, 26785, 38543
##  8: 8, 1473, 28511, 31698
##  9: 9, 11200, 18048, 20814, 32992
##  10: 10, 1763, 15291, 31088, 37014&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, this starts to have problems over really large datasets, because the total
number of comparisons to be done still rapidly increase besides the use of spatial
indexes. A test done by changing the number of points in the above example in the
range 25000 - 475000 shows for example this kind of behavior, for two different
values of maxdist (500 and 2000 m):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/speeding_up/Rplot01.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On the test dataset, the relationships are almost perfectly quadratic (due to the
uniform distribution of points). Extrapolating them to the 12 Million points dataset
of the OP, we would get an execution time of about 14 hours for maxdist = 500, and
a staggering 3.5 days formaxdist = 2000. Still doable, but not ideal…&lt;/p&gt;
&lt;p&gt;My suggestion to the OP was therefore to “split” the points in chunks based on the
x-coordinate and then work on a per-split basis, eventually assigning each chunk
to a different core within a parallellized cycle.&lt;/p&gt;
&lt;p&gt;In the end, I got curious and decided to give it a go to see what kind of performance
improvement it was possible to obtain with that kind of approach. You can find results
of some tests below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-possible-solution-speeding-up-computation-by-combining-data.table-and-sf_intersect&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A (possible) solution: Speeding up computation by combining &lt;code&gt;data.table&lt;/code&gt; and &lt;code&gt;sf_intersect&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;&lt;br&gt;
The idea here is to use a simple divide-and-conquer approach.&lt;/p&gt;
&lt;p&gt;We first split the total spatial extent of the dataset in a certain number of regular quadrants. We then iterate over the quadrants and for each one we:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Extract the points contained into the quadrant and apply a buffer to them;&lt;/li&gt;
&lt;li&gt;Extract the points contained in a slightly larger area, computed by expanding
the quadrant by an amount equal to the maximum distance for which we want to
identify the “neighbors”;&lt;/li&gt;
&lt;li&gt;Compute and save the intersection between the buffered points and the points
contained in the “expanded” quadrant.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“Graphically”, this translates to exploring the dataset like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/speeding_up/animation2.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;, where the points included in the current “quadrant” are shown in green and the additional points needed to perform the analysis for that quadrant are shown in red.&lt;/p&gt;
&lt;p&gt;Provided that the subsetting operations do not introduce an excessive overhead (i.e., they are fast enough…) this should provide a performance boost, because it should consistently reduce the total number of comparisons to be done.&lt;/p&gt;
&lt;p&gt;Now, every “R” expert will tell you that if you need to perform fast subsetting over large datasets the way to go is to use properly indexeddata.tables, which provide lightning-speed subsetting capabilities.&lt;/p&gt;
&lt;p&gt;So, let’s see how we could code this in a functions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;points_in_distance &amp;lt;- function(in_pts,
                               maxdist,
                               ncuts = 10) {

  require(data.table)
  require(sf)
  # convert points to data.table and create a unique identifier
  pts &amp;lt;-  data.table(in_pts)
  pts &amp;lt;- pts[, or_id := 1:dim(in_pts)[1]]

  # divide the extent in quadrants in ncuts*ncuts quadrants and assign each
  # point to a quadrant, then create the index over &amp;quot;x&amp;quot; to speed-up
  # the subsetting
  range_x  &amp;lt;- range(pts$x)
  limits_x &amp;lt;-(range_x[1] + (0:ncuts)*(range_x[2] - range_x[1])/ncuts)
  range_y  &amp;lt;- range(pts$y)
  limits_y &amp;lt;- range_y[1] + (0:ncuts)*(range_y[2] - range_y[1])/ncuts
  pts[, `:=`(xcut =  as.integer(cut(x, ncuts, labels = 1:ncuts)),
             ycut = as.integer(cut(y, ncuts, labels = 1:ncuts)))]  %&amp;gt;%
    setkey(x)

  results &amp;lt;- list()
  count &amp;lt;- 0
  # start cycling over quadrants
  for (cutx in seq_len(ncuts)) {

    # get the points included in a x-slice extended by `maxdist`, and build
    # an index over y to speed-up subsetting in the inner cycle
    min_x_comp    &amp;lt;- ifelse(cutx == 1,
                            limits_x[cutx],
                            (limits_x[cutx] - maxdist))
    max_x_comp    &amp;lt;- ifelse(cutx == ncuts,
                            limits_x[cutx + 1],
                            (limits_x[cutx + 1] + maxdist))
    subpts_x &amp;lt;- pts[x &amp;gt;= min_x_comp &amp;amp; x &amp;lt; max_x_comp] %&amp;gt;%
      setkey(y)

    for (cuty in seq_len(ncuts)) {
      count &amp;lt;- count + 1

      # subset over subpts_x to find the final set of points needed for the
      # comparisons
      min_y_comp  &amp;lt;- ifelse(cuty == 1,
                            limits_y[cuty],
                            (limits_y[cuty] - maxdist))
      max_y_comp  &amp;lt;- ifelse(cuty == ncuts,
                            limits_y[cuty + 1],
                            (limits_y[cuty + 1] + maxdist))
      subpts_comp &amp;lt;- subpts_x[y &amp;gt;= min_y_comp &amp;amp; y &amp;lt; max_y_comp]

      # subset over subpts_comp to get the points included in a x/y chunk,
      # which &amp;quot;neighbours&amp;quot; we want to find. Then buffer them by maxdist.
      subpts_buf &amp;lt;- subpts_comp[ycut == cuty &amp;amp; xcut == cutx] %&amp;gt;%
        sf::st_as_sf() %&amp;gt;% 
        sf::st_buffer(maxdist)

      # retransform to sf since data.tables lost the geometric attrributes
      subpts_comp &amp;lt;- sf::st_as_sf(subpts_comp)

      # compute the intersection and save results in a element of &amp;quot;results&amp;quot;.
      # For each point, save its &amp;quot;or_id&amp;quot; and the &amp;quot;or_ids&amp;quot; of the points within &amp;quot;dist&amp;quot;
      inters &amp;lt;- sf::st_intersects(subpts_buf, subpts_comp)

      # save results
      results[[count]] &amp;lt;- data.table(
        id = subpts_buf$or_id,
        int_ids = lapply(inters, FUN = function(x) subpts_comp$or_id[x]))
    }
  }
  data.table::rbindlist(results)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function takes as input a points sf object, a target distance and a number of “cuts” to use to divide the extent in quadrants, and provides in output a data frame in which, for each original point, the “ids” of the points within maxdist are reported in the int_ids list column.&lt;/p&gt;
&lt;p&gt;Now, let’s see if this works:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pts &amp;lt;- data.frame(x = runif(20000, 0, 100000),
                  y = runif(20000, 0, 100000),
                  id = 1:20000) %&amp;gt;%
  st_as_sf(coords = c(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;), remove = FALSE)
maxdist &amp;lt;- 2000
out &amp;lt;- points_in_distance(pts, maxdist = maxdist, ncut = 10)
head(out)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       id                                 int_ids
## 1:  5830  5830, 9068,10102, 5782,10062, 1188,...
## 2:  9068  5830, 9068,10102, 5782, 1188,15701,...
## 3: 10102  5830, 9068,10102, 5782,10062, 1188,...
## 4:  5989  5989, 7085,18143,  209, 5751, 5130,...
## 5:  5782  5830, 9068,10102, 5782,10062, 1188,...
## 6: 10062  5830,10102, 5782,10062,17566,15701,...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get a random point
sel_id  &amp;lt;- sample(pts$id,1)
pt_sel  &amp;lt;- pts[sel_id, ]
pt_buff &amp;lt;- pt_sel %&amp;gt;%  sf::st_buffer(maxdist)
# get ids of points within maxdist
id_inters &amp;lt;- unlist(out[id == sel_id, ]$int_ids)
pt_inters &amp;lt;- pts[id_inters,]

#plot results
plot &amp;lt;- ggplot(pt_buff)  + theme_light() +
  geom_point(data = pts, aes(x = x, y = y), size = 1) +
  geom_sf(col = &amp;quot;blue&amp;quot;, size = 1.2, fill = &amp;quot;transparent&amp;quot;) +
  geom_sf(data = pt_inters, col = &amp;quot;red&amp;quot;, size = 1.5) +
  geom_point(data = pt_sel, aes(x = x, y = y), size = 2, col = &amp;quot;green&amp;quot;) +
  xlim(st_bbox(pt_buff)[1] - maxdist, st_bbox(pt_buff)[3] + maxdist) +
  ylim(st_bbox(pt_buff)[2] - maxdist, st_bbox(pt_buff)[4] + maxdist) + 
  ggtitle(paste0(&amp;quot;id = &amp;quot;, sel_id, &amp;quot; - Number of points within distance = &amp;quot;, 
                 length(id_inters)))
plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-20-speeding-up-spatial-analyses-by-integrating-sf-and-data-table-a-test-case_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;336&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So far, so good&lt;/strong&gt;. Now, let’s do the same exercise with varying number of points
to see how it behaves in term of speed:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/speeding_up/Rplot3.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Already not bad! In particular for the maxdist = 2000 case, we get a quite large
speed improvement!&lt;/p&gt;
&lt;p&gt;However, a nice thing about the points_in_distance approach is that &lt;strong&gt;it is easily
parallelizable&lt;/strong&gt;. All is needed is to change some lines of the function &lt;em&gt;so that the
outer loop over the x “chunks” exploits a parallel backend&lt;/em&gt; of some kind. (You can
find an example implementation exploiting foreach in &lt;a href=&#34;https://gist.github.com/lbusett/247dc9b0b6bed04ac1b45c03999be348&#34;&gt;this gist&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;On a not-particularly-fast PC, using a 6-cores parallelization leads to this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/speeding_up/Rplot4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking good! Some more skilled programmer could probably squeeze out even more
speed from it by some additional data.table magic, but the improvement is very
noticeable.&lt;/p&gt;
&lt;p&gt;In terms of execution time, extrapolating again to the “infamous” 12 Million
points dataset, this would be what we get:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Method
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Maxdist
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Expected completion time (hours)
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
st_intersect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;15&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
points_in_distance - serial
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;2.5&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
points_in_distance - parallel
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.57&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
st_intersect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    color: red !important;&#34;&gt;85&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
points_in_distance - serial
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;15.2&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
points_in_distance - parallel
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;3.18&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So, we get a 5-6X speed improvement already on the “serial” implementation, and
another 5X thanks to parallelization over 6 cores! On themaxdist = 2000 case,
this means going &lt;strong&gt;from more than 3 days to about 3 hours&lt;/strong&gt;. And if we had more cores
and RAM to throw at it, it would finish in minutes!&lt;/p&gt;
&lt;p&gt;###&lt;strong&gt;Nice!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;##Final Notes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;timings shown here are merely indicative&lt;/strong&gt;, and related to the particular
test-dataset we built. On a less uniformly distributed dataset I would expect a
lower speed improvement.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Some time is “wasted” because sf does not (yet) extend data.tables&lt;/strong&gt;, making it
necessary to recreate sf objects from thedata.table subsets.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The parallel implementation is quick-and-dirty, and &lt;strong&gt;it is a bit of a memory-hog&lt;/strong&gt;!
Be careful before throwing at it 25 processors!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Speed is influenced in a non-trivial way by the number of “cuts”&lt;/strong&gt; used to
subdivide the spatial extent. There may be a sweet-spot related to points
distribution and maxdist allowing reaching maximum speed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A similar approach for parallelization could exploit repeatedly “cropping” the
original sf points object over the extent of the chunk/extended chunk. The
&lt;strong&gt;data.table approach seems however to be faster&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;That’s all! Hope you liked this (rather long) post!&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
      </item>
    
    <item>
      <title>A Critical Comparison of Remote Sensing Leaf Area Index Estimates over Rice-Cultivated Areas: From Sentinel-2 and Landsat-7/8 to MODIS, GEOV1 and EUMETSAT Polar System</title>
      <link>/publication/2018-01-01_a-critical_comparison/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/2018-01-01_a-critical_comparison/</guid>
      <description></description>
      </item>
    
    <item>
      <title>A high-resolution, integrated system for rice yield forecasting at district level</title>
      <link>/publication/2018-01-01_a-high-resolution/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/2018-01-01_a-high-resolution/</guid>
      <description></description>
      </item>
    
    <item>
      <title>Analysing spatial–temporal changes in rice cultivation practices in the Senegal River Valley using MODIS time-series and the PhenoRice algorithm</title>
      <link>/publication/2018-01-01_analysing_spatial_temp/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/2018-01-01_analysing_spatial_temp/</guid>
      <description></description>
      </item>
    
    <item>
      <title>Assessment of Water Management Changes in the Italian Rice Paddies from 2000 to 2016 Using Satellite Data: A Contribution to Agro-Ecological Studies</title>
      <link>/publication/2018-01-01_assessment_of_water_/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/2018-01-01_assessment_of_water_/</guid>
      <description></description>
      </item>
    
  </channel>
</rss>
